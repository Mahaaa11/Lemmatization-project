{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers datasets accelerate --quiet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote quran_sentences.jsonl with 6216 sentences\n"
     ]
    }
   ],
   "source": [
    "# parse_quran_morph.py\n",
    "import re, json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "INPUT = \"quranic-corpus-morphology-0.4.txt\"\n",
    "OUT = \"quran_sentences.jsonl\"\n",
    "\n",
    "# regex to capture lines with token entries:\n",
    "# format: (loc) <FORM> <TAG> <FEATURES... LEM:... >\n",
    "LINE_RE = re.compile(r'^\\([^)]+\\)\\s+([^\\t\\s]+)\\t([^\\t]+)\\t(.+)$')\n",
    "\n",
    "def extract_lem_from_features(feat):\n",
    "    # find LEM:... up to '|' or end\n",
    "    m = re.search(r'LEM:([^|\\s)]+)', feat)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    return None\n",
    "\n",
    "# We will group tokens by verse location (chapter:verse:..)\n",
    "LOC_RE = re.compile(r'^\\((\\d+:\\d+):')  # captures chapter:verse start\n",
    "# Actually entries look like (1:1:1:1) etc. We'll use chapter:verse pair\n",
    "LOC_FULL_RE = re.compile(r'^\\((\\d+):(\\d+):')\n",
    "\n",
    "sent_map = defaultdict(list)  # key (chapter,verse) -> list of (token,lemma)\n",
    "\n",
    "with open(INPUT, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        m = LINE_RE.match(line)\n",
    "        if not m:\n",
    "            continue\n",
    "        form, tag, feats = m.group(1), m.group(2), m.group(3)\n",
    "        # normalize form: keep Arabic/buckwalter form as-is\n",
    "        # extract lemma\n",
    "        lem = extract_lem_from_features(feats)\n",
    "        # determine location key\n",
    "        loc_m = LOC_FULL_RE.match(line)\n",
    "        if loc_m:\n",
    "            chap = loc_m.group(1)\n",
    "            verse = loc_m.group(2)\n",
    "        else:\n",
    "            chap = \"0\"; verse = \"0\"\n",
    "        if lem is None:\n",
    "            continue\n",
    "        # store\n",
    "        sent_map[(chap,verse)].append((form, lem))\n",
    "\n",
    "# write JSONL by verse (preserve verse order roughly)\n",
    "out = Path(OUT)\n",
    "with out.open(\"w\", encoding=\"utf-8\") as fo:\n",
    "    for k in sorted(sent_map.keys(), key=lambda x: (int(x[0]), int(x[1]))):\n",
    "        toks = [t for t,l in sent_map[k]]\n",
    "        lems = [l for t,l in sent_map[k]]\n",
    "        fo.write(json.dumps({\"tokens\": toks, \"labels\": lems}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Wrote\", OUT, \"with\", len(sent_map), \"sentences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged labels: 8102\n"
     ]
    }
   ],
   "source": [
    "# merge_aux_csv.py\n",
    "import csv, json\n",
    "from pathlib import Path\n",
    "\n",
    "CSV = \"quran_part3_unique.csv\"\n",
    "Q_SENT = \"quran_sentences.jsonl\"\n",
    "OUT_LABEL = \"merged_label2id.json\"\n",
    "\n",
    "# load existing labels from Quran sentences\n",
    "labels_set = set()\n",
    "with open(Q_SENT, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        for l in obj['labels']:\n",
    "            labels_set.add(l)\n",
    "\n",
    "# read csv pairs and add lemmas to label set\n",
    "with open(CSV, encoding='utf-8') as f:\n",
    "    # try to detect delimiter simply\n",
    "    sample = f.read(2048); f.seek(0)\n",
    "    dialect = csv.Sniffer().sniff(sample)\n",
    "    reader = csv.reader(f, dialect)\n",
    "    for row in reader:\n",
    "        if not row: continue\n",
    "        if len(row) >= 2:\n",
    "            word, lemma = row[0].strip(), row[1].strip()\n",
    "        else:\n",
    "            # if single column, split last token as lemma\n",
    "            parts = row[0].strip().split()\n",
    "            if len(parts) < 2: continue\n",
    "            word, lemma = \" \".join(parts[:-1]), parts[-1]\n",
    "        if lemma:\n",
    "            labels_set.add(lemma)\n",
    "\n",
    "labels = sorted(labels_set)\n",
    "label2id = {l:i for i,l in enumerate(labels)}\n",
    "\n",
    "with open(OUT_LABEL, \"w\", encoding='utf-8') as f:\n",
    "    json.dump(label2id, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Merged labels:\", len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created splits in data_combined\n"
     ]
    }
   ],
   "source": [
    "# split_create.py\n",
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "Q_SENT = \"quran_sentences.jsonl\"\n",
    "OUT_DIR = Path(\"data_combined\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "random.seed(42)\n",
    "\n",
    "lines = [json.loads(l) for l in open(Q_SENT, encoding='utf-8')]\n",
    "random.shuffle(lines)\n",
    "\n",
    "n = len(lines)\n",
    "train = lines[:int(0.8*n)]\n",
    "val   = lines[int(0.8*n):int(0.9*n)]\n",
    "test  = lines[int(0.9*n):]\n",
    "\n",
    "for name,chunk in [(\"train\",train),(\"val\",val),(\"test\",test)]:\n",
    "    with open(OUT_DIR/f\"{name}.jsonl\",\"w\",encoding='utf-8') as fo:\n",
    "        for s in chunk:\n",
    "            fo.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n",
    "print(\"Created splits in\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded label map: 8102\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a821a1131b74683ae33ca62f61749b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4972 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353367bc29c54a99af27ec49dd11c73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/622 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f257c03560d245dd984c779d6f18a4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/622 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-2411075110.py:134: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type dict that is 294992 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type dict that is 207616 bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2177' max='2177' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2177/2177 17:28, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.008100</td>\n",
       "      <td>2.559235</td>\n",
       "      <td>0.671769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.883300</td>\n",
       "      <td>1.716960</td>\n",
       "      <td>0.779511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.354400</td>\n",
       "      <td>1.373241</td>\n",
       "      <td>0.830291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.075900</td>\n",
       "      <td>1.196966</td>\n",
       "      <td>0.854136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.892200</td>\n",
       "      <td>1.103389</td>\n",
       "      <td>0.864145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>1.052548</td>\n",
       "      <td>0.871357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.748300</td>\n",
       "      <td>1.039040</td>\n",
       "      <td>0.873271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete! Model saved to lemmatizer_final/\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "#  Arabert\n",
    "# ==============================================\n",
    "\n",
    "!pip install -U transformers datasets accelerate --quiet\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Load label map\n",
    "# -----------------------------\n",
    "with open(\"merged_label2id.json\", encoding=\"utf-8\") as f:\n",
    "    label2id = json.load(f)\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "print(\"Loaded label map:\", len(label2id))\n",
    "\n",
    "# -----------------------------\n",
    "# Load dataset (train/val/test)\n",
    "# -----------------------------\n",
    "def load_jsonl(path):\n",
    "    out = []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            out.append(json.loads(line))\n",
    "    return out\n",
    "\n",
    "train = Dataset.from_list(load_jsonl(\"data_combined/train.jsonl\"))\n",
    "val   = Dataset.from_list(load_jsonl(\"data_combined/val.jsonl\"))\n",
    "test  = Dataset.from_list(load_jsonl(\"data_combined/test.jsonl\"))\n",
    "\n",
    "# -----------------------------\n",
    "# Choose model\n",
    "# -----------------------------\n",
    "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenize + Align Labels\n",
    "# -----------------------------\n",
    "def tokenize_and_align(batch):\n",
    "    enc = tokenizer(\n",
    "        batch[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    aligned_labels = []\n",
    "    for i, labels in enumerate(batch[\"labels\"]):\n",
    "        word_ids = enc.word_ids(batch_index=i)\n",
    "        prev_id = None\n",
    "        label_ids = []\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                label_ids.append(-100)\n",
    "            elif wid != prev_id:\n",
    "                label_ids.append(label2id[labels[wid]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            prev_id = wid\n",
    "        aligned_labels.append(label_ids)\n",
    "\n",
    "    enc[\"labels\"] = aligned_labels\n",
    "    return enc\n",
    "\n",
    "train_tok = train.map(tokenize_and_align, batched=True, remove_columns=[\"tokens\",\"labels\"])\n",
    "val_tok   = val.map(tokenize_and_align,   batched=True, remove_columns=[\"tokens\",\"labels\"])\n",
    "test_tok  = test.map(tokenize_and_align,  batched=True, remove_columns=[\"tokens\",\"labels\"])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Load Model\n",
    "# -----------------------------\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# TrainingArguments (VALID)\n",
    "# -----------------------------\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"lemmatizer_output\",\n",
    "    eval_strategy=\"epoch\",   # <--- VALID in 4.57.3\n",
    "    save_strategy=\"epoch\",         # <--- VALID\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=7,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    true_preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    for p_row, l_row in zip(preds, labels):\n",
    "        for p, l in zip(p_row, l_row):\n",
    "            if l != -100:\n",
    "                true_preds.append(p)\n",
    "                true_labels.append(l)\n",
    "\n",
    "    return {\"accuracy\": (np.array(true_preds) == np.array(true_labels)).mean()}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Trainer\n",
    "# -----------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Train\n",
    "# -----------------------------\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"lemmatizer_final\")\n",
    "tokenizer.save_pretrained(\"lemmatizer_final\")\n",
    "\n",
    "print(\"\\nTraining complete! Model saved to lemmatizer_final/\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
